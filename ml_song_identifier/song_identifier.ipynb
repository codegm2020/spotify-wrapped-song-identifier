{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Music Genre Classification with a PyTorch CNN\n",
        "This notebook walks through the process of building a music genre classifier using Mel-spectrograms and a Convolutional Neural Network (CNN).\n",
        "### Steps:\n",
        "1.  **Data Preparation**: Load audio files and transform them into Mel-spectrograms.\n",
        "2.  **Dataset & DataLoader**: Create a custom PyTorch dataset to handle the data.\n",
        "3.  **Model Building**: Define and instantiate the CNN model.\n",
        "4.  **Training**: Train the model on the spectrogram data.\n",
        "5.  **Evaluation**: Evaluate the model's performance on a validation set."
      ],
      "metadata": {
        "id": "KMLYYpshYWwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TurDS96AYelD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import our custom CNN model\n",
        "from model.cnn_model import GenreCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preprocessing: From Audio to Spectrogram\n",
        "\n",
        "We will process the raw `.wav` files from the GTZAN dataset. For each 30-second audio clip, we'll generate a Mel-spectrogram, which is a visual representation suitable for a CNN.\n",
        "\n",
        "**Important:** Update the `DATASET_PATH` to the location of your GTZAN dataset's `genres_original` folder."
      ],
      "metadata": {
        "id": "1veREFxuY2zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"path/to/your/genres_original\" # <--- IMPORTANT: CHANGE THIS\n",
        "JSON_PATH = \"data.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30 # seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n",
        "\n",
        "def save_mel_spectrograms(dataset_path, json_path, n_mels=128, n_fft=2048, hop_length=512):\n",
        "    data = {\n",
        "        \"mappings\": [],\n",
        "        \"labels\": [],\n",
        "        \"mel_spectrograms\": []\n",
        "    }\n",
        "\n",
        "    num_samples_per_segment = int(SAMPLES_PER_TRACK / 10) # 10 segments per track\n",
        "\n",
        "    for i, (dirpath, _, filenames) in enumerate(os.walk(dataset_path)):\n",
        "        if dirpath is not dataset_path:\n",
        "            # Save the genre label\n",
        "            genre_label = os.path.basename(dirpath)\n",
        "            data[\"mappings\"].append(genre_label)\n",
        "            print(f\"\\nProcessing: {genre_label}\")\n",
        "\n",
        "            # Process all audio files in the genre sub-folder\n",
        "            for f in tqdm(filenames):\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                try:\n",
        "                    signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "                    # Process segments of the audio file\n",
        "                    for s in range(10):\n",
        "                        start_sample = num_samples_per_segment * s\n",
        "                        finish_sample = start_sample + num_samples_per_segment\n",
        "\n",
        "                        # Generate Mel-spectrogram\n",
        "                        mel_spec = librosa.feature.melspectrogram(\n",
        "                            y=signal[start_sample:finish_sample],\n",
        "                            sr=sr,\n",
        "                            n_fft=n_fft,\n",
        "                            n_mels=n_mels,\n",
        "                            hop_length=hop_length\n",
        "                        )\n",
        "                        log_mel_spec = librosa.power_to_db(mel_spec)\n",
        "\n",
        "                        if log_mel_spec.shape == (n_mels, 130): # Ensure consistent shape\n",
        "                            data[\"mel_spectrograms\"].append(log_mel_spec.tolist())\n",
        "                            data[\"labels\"].append(i - 1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not process {file_path}: {e}\")\n",
        "\n",
        "    # Save to JSON file\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n",
        "    print(\"\\nData successfully saved to data.json\")\n",
        "\n",
        "# Run the function only if the JSON file doesn't exist\n",
        "if not os.path.exists(JSON_PATH):\n",
        "    save_mel_spectrograms(DATASET_PATH, JSON_PATH)\n",
        "else:\n",
        "    print(\"data.json already exists. Skipping preprocessing.\")"
      ],
      "metadata": {
        "id": "19W9GBgVZFQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample spectrogram and display it\n",
        "with open(JSON_PATH, \"r\") as fp:\n",
        "    sample_data = json.load(fp)\n",
        "\n",
        "sample_spectrogram = np.array(sample_data[\"mel_spectrograms\"][0])\n",
        "genre_map = sample_data[\"mappings\"]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(sample_spectrogram, sr=SAMPLE_RATE, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(f'Mel-spectrogram for a \"{genre_map[0]}\" track')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W4q7mjxrZQQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating a Custom PyTorch Dataset\n",
        "\n",
        "We'll create a custom `Dataset` class to load our data and prepare it for the `DataLoader`."
      ],
      "metadata": {
        "id": "MJogk4jjZXm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenreDataset(Dataset):\n",
        "    def __init__(self, X, y, device):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1).to(device) # Add channel dimension\n",
        "        self.y = torch.tensor(y, dtype=torch.long).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def load_data(json_path):\n",
        "    with open(json_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "    X = np.array(data[\"mel_spectrograms\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "    return X, y\n",
        "\n",
        "# Load data and split into train/validation sets\n",
        "X, y = load_data(JSON_PATH)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = GenreDataset(X_train, y_train, device)\n",
        "val_dataset = GenreDataset(X_val, y_val, device)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "1bJHi6OIZZm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training the CNN Model\n",
        "\n",
        "Now we define the training loop, loss function, and optimizer, and then train our model."
      ],
      "metadata": {
        "id": "NS8WhYqvZeVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = GenreCNN(num_genres=len(genre_map)).to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "print(\"Finished Training\")\n",
        "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "id": "Ari3dkleZhB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizing Performance"
      ],
      "metadata": {
        "id": "0OUfDDXXZtnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cV4RhaQUZwt_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}